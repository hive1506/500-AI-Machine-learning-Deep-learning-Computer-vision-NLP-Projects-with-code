{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flaskapp draft 9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeovYmh+ABz4fxexdKLw+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hive1506/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/blob/main/flaskapp_draft_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqWbjXBnY03-"
      },
      "source": [
        "#Getting started Wav2Lip API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edY1JyhbZJU",
        "outputId": "08921d1f-09b0-4838-ee03-516e25558f3b"
      },
      "source": [
        "#connect to your google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g5M6W8e7nzwEQ_RcDNONwf0QOboOHSYnSCm1wfY8Hu0YtHDVF3BIJc\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSFI4RIVbWcX",
        "outputId": "52c78345-fd5b-4fc7-b104-362932d5aa13"
      },
      "source": [
        "#clone the code\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git\n",
        "print(\"Successfully cloned!\")\n",
        "\n",
        "#after cloning, copy the dataset in .pth from the drive to the colab files\n",
        "!cp -ri \"/content/gdrive/MyDrive/datasets/Wav2lip/wav2lip_gan.pth\" \"/content/Wav2Lip/checkpoints/\"\n",
        "print(\"Successully copied to files!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Wav2Lip' already exists and is not an empty directory.\n",
            "Successfully cloned!\n",
            "Successully copied to files!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SGYLrBqb3-l",
        "outputId": "622768ff-11d4-4f5b-96cb-39ba17164493"
      },
      "source": [
        "#copy some sample input audio and video files\n",
        "!cp \"/content/gdrive/My Drive/datasets/Wav2lip/tushar.png\" \"/content/gdrive/MyDrive/datasets/Wav2lip/input_audio.wav\" sample_data/\n",
        "print(\"Successfully copied to colab files!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully copied to colab files!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYHqhMtFbvEB",
        "outputId": "23adddaa-d563-4d74-b9e0-d4ed4b1cc9f8"
      },
      "source": [
        "!pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip && pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Collecting librosa==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/6e/0eb0de1c9c4e02df0b40e56f258eb79bd957be79b918511a184268e01720/librosa-0.7.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 6.9MB/s \n",
            "\u001b[?25hCollecting numpy==1.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/eb/4ecf6b13897391cb07a4231e9d9c671b55dfbbf6f4a514a1a0c594f2d8d9/numpy-1.17.1-cp37-cp37m-manylinux1_x86_64.whl (20.3MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3MB 1.4MB/s \n",
            "\u001b[?25hCollecting opencv-contrib-python>=4.2.0.34\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/69/cc755e763f4bb20cb25c7696172e6bb556719faf458c5362f4e54d6cd765/opencv_contrib_python-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (56.3MB)\n",
            "\u001b[K     |████████████████████████████████| 56.4MB 72kB/s \n",
            "\u001b[?25hCollecting opencv-python==4.1.0.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/52/61b9619a7a95a8d809515f68f1441224a07ce1873fd3af5e662851014a55/opencv_python-4.1.0.25-cp37-cp37m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 1.5MB/s \n",
            "\u001b[?25hCollecting torch==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/23/a4b5c189dd624411ec84613b717594a00480282b949e3448d189c4aa4e47/torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9MB)\n",
            "\u001b[K     |████████████████████████████████| 676.9MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/9b/208f48d5a5013bdb0c27a84a02df4fcf5fd24ab5902667c11e554a12b681/torchvision-0.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 39.1MB/s \n",
            "\u001b[?25hCollecting tqdm==4.45.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting numba==0.48\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/dc/5ce4a94d98e8a31cab21b150e23ca2f09a7dd354c06a69f71801ecd890db/numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 33.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.10.3.post1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.3.0->-r requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48->-r requirements.txt (line 8)) (54.1.2)\n",
            "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/10/d02c0ac683fc47ecda3426249509cf771d748b6a2c0e9d5ebbee76a7b80a/llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.7.0->-r requirements.txt (line 1)) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.0->-r requirements.txt (line 1)) (2.20)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.0-cp37-none-any.whl size=1598345 sha256=108124d54c845b1e37f50f5e5f3e5ca70985104c787531ebe2298191a9920c38\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/1d/38/c8ad12fcad67569d8e730c3275be5e581bd589558484a0f881\n",
            "Successfully built librosa\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement numba>=0.49, but you'll have numba 0.48.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pynndescent 0.5.2 has requirement numba>=0.51.2, but you'll have numba 0.48.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, llvmlite, numba, librosa, opencv-contrib-python, opencv-python, torch, torchvision, tqdm\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Found existing installation: librosa 0.8.0\n",
            "    Uninstalling librosa-0.8.0:\n",
            "      Successfully uninstalled librosa-0.8.0\n",
            "  Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed librosa-0.7.0 llvmlite-0.31.0 numba-0.48.0 numpy-1.17.1 opencv-contrib-python-4.5.1.48 opencv-python-4.1.0.25 torch-1.1.0 torchvision-0.3.0 tqdm-4.45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BXBADC_berv",
        "outputId": "8fe8b64c-d974-4829-e86f-43ab6a035eb4"
      },
      "source": [
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/sample_data/s3fd.pth\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-23 09:59:39--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
            "Resolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\n",
            "Connecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89843225 (86M) [application/octet-stream]\n",
            "Saving to: ‘/content/sample_data/s3fd.pth’\n",
            "\n",
            "/content/sample_dat 100%[===================>]  85.68M  16.6MB/s    in 5.9s    \n",
            "\n",
            "2021-03-23 09:59:46 (14.5 MB/s) - ‘/content/sample_data/s3fd.pth’ saved [89843225/89843225]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJcPgtdFmIY6",
        "outputId": "554f9b60-be70-41c0-a9b8-d87ae92d9909"
      },
      "source": [
        "!cd /content/Wav2Lip/ && gdown https://drive.google.com/uc?id=14uiXak-537YUXTNuWBTOVjdLaEWOgGl8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14uiXak-537YUXTNuWBTOVjdLaEWOgGl8\n",
            "To: /content/Wav2Lip/wav2lip_gan.pth\n",
            "436MB [00:03, 135MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iksgVqiFcALP",
        "outputId": "1becdfec-4a64-4e33-93bd-405a0e70d38b"
      },
      "source": [
        "#Run the inference.py file\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/tushar.png\" --audio \"../sample_data/input_audio.wav\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 1\n",
            "(80, 593)\n",
            "Length of mel chunks: 182\n",
            "  0% 0/2 [00:00<?, ?it/s]Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/checkpoints/s3fd-619a316812.pth\n",
            "\n",
            "  0% 0/89843225 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 32768/89843225 [00:00<06:37, 226038.64it/s]\u001b[A\n",
            "  0% 98304/89843225 [00:00<05:38, 265397.95it/s]\u001b[A\n",
            "  0% 212992/89843225 [00:00<04:30, 331034.52it/s]\u001b[A\n",
            "  0% 442368/89843225 [00:00<03:26, 433676.12it/s]\u001b[A\n",
            "  1% 884736/89843225 [00:00<02:32, 583701.08it/s]\u001b[A\n",
            "  2% 1802240/89843225 [00:00<01:49, 801884.96it/s]\u001b[A\n",
            "  4% 3653632/89843225 [00:01<01:17, 1115092.99it/s]\u001b[A\n",
            "  7% 5865472/89843225 [00:01<00:54, 1544210.66it/s]\u001b[A\n",
            "  9% 7716864/89843225 [00:01<00:39, 2096274.04it/s]\u001b[A\n",
            " 11% 9895936/89843225 [00:01<00:28, 2824144.93it/s]\u001b[A\n",
            " 13% 11943936/89843225 [00:01<00:20, 3712745.65it/s]\u001b[A\n",
            " 16% 13959168/89843225 [00:01<00:15, 4755224.71it/s]\u001b[A\n",
            " 18% 16007168/89843225 [00:01<00:12, 5928067.36it/s]\u001b[A\n",
            " 20% 18038784/89843225 [00:02<00:10, 7158489.15it/s]\u001b[A\n",
            " 22% 20168704/89843225 [00:02<00:08, 8447263.29it/s]\u001b[A\n",
            " 25% 22233088/89843225 [00:02<00:07, 9602717.01it/s]\u001b[A\n",
            " 27% 24330240/89843225 [00:02<00:06, 10658046.12it/s]\u001b[A\n",
            " 29% 25731072/89843225 [00:02<00:08, 7757108.53it/s] \u001b[A\n",
            " 31% 28016640/89843225 [00:02<00:06, 9150500.86it/s]\u001b[A\n",
            " 33% 29753344/89843225 [00:03<00:06, 9825769.12it/s]\u001b[A\n",
            " 35% 31866880/89843225 [00:03<00:05, 10867303.89it/s]\u001b[A\n",
            " 38% 33849344/89843225 [00:03<00:04, 11549830.74it/s]\u001b[A\n",
            " 40% 35913728/89843225 [00:03<00:04, 12203439.60it/s]\u001b[A\n",
            " 42% 37945344/89843225 [00:03<00:04, 12657498.69it/s]\u001b[A\n",
            " 44% 39911424/89843225 [00:03<00:03, 12884907.57it/s]\u001b[A\n",
            " 47% 41992192/89843225 [00:03<00:03, 13253398.06it/s]\u001b[A\n",
            " 49% 44007424/89843225 [00:04<00:03, 13400960.44it/s]\u001b[A\n",
            " 51% 46055424/89843225 [00:04<00:03, 13551208.73it/s]\u001b[A\n",
            " 54% 48087040/89843225 [00:04<00:03, 13672542.11it/s]\u001b[A\n",
            " 56% 50053120/89843225 [00:04<00:02, 13596870.92it/s]\u001b[A\n",
            " 58% 52133888/89843225 [00:04<00:02, 13776422.32it/s]\u001b[A\n",
            " 60% 54149120/89843225 [00:04<00:02, 13768148.26it/s]\u001b[A\n",
            " 63% 56213504/89843225 [00:04<00:02, 13867910.39it/s]\u001b[A\n",
            " 65% 58228736/89843225 [00:05<00:02, 13847090.74it/s]\u001b[A\n",
            " 67% 60293120/89843225 [00:05<00:02, 13914937.55it/s]\u001b[A\n",
            " 69% 62308352/89843225 [00:05<00:01, 13859762.96it/s]\u001b[A\n",
            " 72% 64356352/89843225 [00:05<00:01, 13913290.21it/s]\u001b[A\n",
            " 74% 66306048/89843225 [00:05<00:01, 13732180.39it/s]\u001b[A\n",
            " 76% 68386816/89843225 [00:05<00:01, 13863637.79it/s]\u001b[A\n",
            " 78% 70402048/89843225 [00:06<00:01, 13834775.13it/s]\u001b[A\n",
            " 81% 72450048/89843225 [00:06<00:01, 13886037.50it/s]\u001b[A\n",
            " 83% 74465280/89843225 [00:06<00:01, 13854298.18it/s]\u001b[A\n",
            " 85% 76431360/89843225 [00:06<00:00, 13727123.48it/s]\u001b[A\n",
            " 87% 78495744/89843225 [00:06<00:00, 13825385.77it/s]\u001b[A\n",
            " 90% 80527360/89843225 [00:06<00:00, 13839083.89it/s]\u001b[A\n",
            " 92% 82558976/89843225 [00:06<00:00, 13831775.23it/s]\u001b[A\n",
            " 94% 84508672/89843225 [00:07<00:00, 13678238.05it/s]\u001b[A\n",
            " 96% 86523904/89843225 [00:07<00:00, 13700640.19it/s]\u001b[A\n",
            "100% 89843225/89843225 [00:07<00:00, 12256246.20it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:02<00:00,  2.98s/it]\n",
            "Load checkpoint from: checkpoints/wav2lip_gan.pth\n",
            "Model loaded\n",
            "100% 2/2 [00:48<00:00, 24.05s/it]\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
            "\u001b[0mInput #0, wav, from '../sample_data/input_audio.wav':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:07.40, bitrate: 1536 kb/s\n",
            "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
            "Input #1, avi, from 'temp/result.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.26.101\n",
            "  Duration: 00:00:07.28, start: 0.000000, bitrate: 473 kb/s\n",
            "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 512x512 [SAR 1:1 DAR 1:1], 465 kb/s, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
            "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
            "\u001b[0m\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mprofile High, level 3.0\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'results/result_voice.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 512x512 [SAR 1:1 DAR 1:1], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 aac\n",
            "frame=  182 fps=137 q=-1.0 Lsize=     191kB time=00:00:07.40 bitrate= 211.7kbits/s speed=5.57x    \n",
            "video:65kB audio:120kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.634161%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mframe I:1     Avg QP:16.12  size: 18112\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mframe P:65    Avg QP:17.79  size:   528\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mframe B:116   Avg QP:19.91  size:   115\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mconsecutive B-frames: 10.4%  9.9% 11.5% 68.1%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mmb I  I16..4: 36.8% 60.8%  2.3%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mmb P  I16..4:  0.0%  0.4%  0.0%  P16..4:  4.2%  1.8%  1.1%  0.0%  0.0%    skip:92.4%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mmb B  I16..4:  0.0%  0.1%  0.0%  B16..8:  4.2%  0.3%  0.0%  direct: 0.0%  skip:95.3%  L0:57.5% L1:39.9% BI: 2.6%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0m8x8 transform intra:68.0% inter:89.2%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mcoded y,uvDC,uvAC intra: 49.4% 30.6% 9.5% inter: 1.1% 0.9% 0.0%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mi16 v,h,dc,p: 56% 22% 18%  4%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 16% 40%  4%  3%  2%  3%  4%  5%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 18% 18% 12%  3%  4%  4%  6% 14%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mi8c dc,h,v,p: 62% 14% 22%  2%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mref P L0: 65.1% 11.3% 14.5%  9.1%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mref B L0: 75.7% 19.4%  4.9%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mref B L1: 97.9%  2.1%\n",
            "\u001b[1;36m[libx264 @ 0x558048610d00] \u001b[0mkb/s:72.24\n",
            "\u001b[1;36m[aac @ 0x55804866e000] \u001b[0mQavg: 574.082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBoenbr-kd_Q"
      },
      "source": [
        "Copy the following code in inference.py file\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "from os import listdir, path\n",
        "import numpy as np\n",
        "import scipy, cv2, os, sys, argparse, audio\n",
        "import json, subprocess, random, string\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import torch, face_detection\n",
        "from models import Wav2Lip\n",
        "import platform\n",
        "import easydict\n",
        "\n",
        "'''\n",
        "parser = argparse.ArgumentParser(description='Inference code to lip-sync videos in the wild using Wav2Lip models')\n",
        "\n",
        "parser.add_argument('--checkpoint_path', type=str, \n",
        "\t\t\t\t\thelp='Name of saved checkpoint to load weights from', required=True)\n",
        "\n",
        "parser.add_argument('--face', type=str, \n",
        "\t\t\t\t\thelp='Filepath of video/image that contains faces to use', required=True)\n",
        "parser.add_argument('--audio', type=str, \n",
        "\t\t\t\t\thelp='Filepath of video/audio file to use as raw audio source', required=True)\n",
        "parser.add_argument('--outfile', type=str, help='Video path to save result. See default for an e.g.', \n",
        "\t\t\t\t\t\t\t\tdefault='results/result_voice.mp4')\n",
        "\n",
        "parser.add_argument('--static', type=bool, \n",
        "\t\t\t\t\thelp='If True, then use only first video frame for inference', default=False)\n",
        "parser.add_argument('--fps', type=float, help='Can be specified only if input is a static image (default: 25)', \n",
        "\t\t\t\t\tdefault=25., required=False)\n",
        "\n",
        "parser.add_argument('--pads', nargs='+', type=int, default=[0, 10, 0, 0], \n",
        "\t\t\t\t\thelp='Padding (top, bottom, left, right). Please adjust to include chin at least')\n",
        "\n",
        "parser.add_argument('--face_det_batch_size', type=int, \n",
        "\t\t\t\t\thelp='Batch size for face detection', default=16)\n",
        "parser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip model(s)', default=128)\n",
        "\n",
        "parser.add_argument('--resize_factor', default=1, type=int, \n",
        "\t\t\thelp='Reduce the resolution by this factor. Sometimes, best results are obtained at 480p or 720p')\n",
        "\n",
        "parser.add_argument('--crop', nargs='+', type=int, default=[0, -1, 0, -1], \n",
        "\t\t\t\t\thelp='Crop video to a smaller region (top, bottom, left, right). Applied after resize_factor and rotate arg. ' \n",
        "\t\t\t\t\t'Useful if multiple face present. -1 implies the value will be auto-inferred based on height, width')\n",
        "\n",
        "parser.add_argument('--box', nargs='+', type=int, default=[-1, -1, -1, -1], \n",
        "\t\t\t\t\thelp='Specify a constant bounding box for the face. Use only as a last resort if the face is not detected.'\n",
        "\t\t\t\t\t'Also, might work only if the face is not moving around much. Syntax: (top, bottom, left, right).')\n",
        "\n",
        "parser.add_argument('--rotate', default=False, action='store_true',\n",
        "\t\t\t\t\thelp='Sometimes videos taken from a phone can be flipped 90deg. If true, will flip video right by 90deg.'\n",
        "\t\t\t\t\t'Use if you get a flipped result, despite feeding a normal looking video')\n",
        "\n",
        "parser.add_argument('--nosmooth', default=False, action='store_true',\n",
        "\t\t\t\t\thelp='Prevent smoothing face detections over a short temporal window')\n",
        "'''\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "  \"checkpoint_path\": 'checkpoints/wav2lip_gan.pth',\n",
        "  \"face\": \"../sample_data/tushar.png\",\n",
        "  \"audio\": \"../sample_data/input_audio.wav\",\n",
        "  \"outfile\": 'results/result_voice.mp4',\n",
        "  \"static\": False,\n",
        "  \"fps\": 25.,\n",
        "  \"pads\": [0, 10, 0, 0],\n",
        "  \"face_det_batch_size\": 16,\n",
        "  \"wav2lip_batch_size\": 128,\n",
        "  \"resize_factor\": 1,\n",
        "  \"crop\": [0, -1, 0, -1],\n",
        "  \"box\": [-1, -1, -1, -1],\n",
        "  \"rotate\": False,\n",
        "  \"nosmooth\": False,\n",
        "})\n",
        "\n",
        "args.img_size = 96\n",
        "\n",
        "if os.path.isfile(args.face) and args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
        "\targs.static = True\n",
        "\n",
        "def get_smoothened_boxes(boxes, T):\n",
        "\tfor i in range(len(boxes)):\n",
        "\t\tif i + T > len(boxes):\n",
        "\t\t\twindow = boxes[len(boxes) - T:]\n",
        "\t\telse:\n",
        "\t\t\twindow = boxes[i : i + T]\n",
        "\t\tboxes[i] = np.mean(window, axis=0)\n",
        "\treturn boxes\n",
        "\n",
        "def face_detect(images):\n",
        "\tdetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
        "\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\n",
        "\n",
        "\tbatch_size = args.face_det_batch_size\n",
        "\t\n",
        "\twhile 1:\n",
        "\t\tpredictions = []\n",
        "\t\ttry:\n",
        "\t\t\tfor i in tqdm(range(0, len(images), batch_size)):\n",
        "\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
        "\t\texcept RuntimeError:\n",
        "\t\t\tif batch_size == 1: \n",
        "\t\t\t\traise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
        "\t\t\tbatch_size //= 2\n",
        "\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
        "\t\t\tcontinue\n",
        "\t\tbreak\n",
        "\n",
        "\tresults = []\n",
        "\tpady1, pady2, padx1, padx2 = args.pads\n",
        "\tfor rect, image in zip(predictions, images):\n",
        "\t\tif rect is None:\n",
        "\t\t\tcv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
        "\t\t\traise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
        "\n",
        "\t\ty1 = max(0, rect[1] - pady1)\n",
        "\t\ty2 = min(image.shape[0], rect[3] + pady2)\n",
        "\t\tx1 = max(0, rect[0] - padx1)\n",
        "\t\tx2 = min(image.shape[1], rect[2] + padx2)\n",
        "\t\t\n",
        "\t\tresults.append([x1, y1, x2, y2])\n",
        "\n",
        "\tboxes = np.array(results)\n",
        "\tif not args.nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n",
        "\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
        "\n",
        "\tdel detector\n",
        "\treturn results \n",
        "\n",
        "def datagen(frames, mels):\n",
        "\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "\tif args.box[0] == -1:\n",
        "\t\tif not args.static:\n",
        "\t\t\tface_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n",
        "\t\telse:\n",
        "\t\t\tface_det_results = face_detect([frames[0]])\n",
        "\telse:\n",
        "\t\tprint('Using the specified bounding box instead of face detection...')\n",
        "\t\ty1, y2, x1, x2 = args.box\n",
        "\t\tface_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]\n",
        "\n",
        "\tfor i, m in enumerate(mels):\n",
        "\t\tidx = 0 if args.static else i%len(frames)\n",
        "\t\tframe_to_save = frames[idx].copy()\n",
        "\t\tface, coords = face_det_results[idx].copy()\n",
        "\n",
        "\t\tface = cv2.resize(face, (args.img_size, args.img_size))\n",
        "\t\t\t\n",
        "\t\timg_batch.append(face)\n",
        "\t\tmel_batch.append(m)\n",
        "\t\tframe_batch.append(frame_to_save)\n",
        "\t\tcoords_batch.append(coords)\n",
        "\n",
        "\t\tif len(img_batch) >= args.wav2lip_batch_size:\n",
        "\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "\t\t\timg_masked = img_batch.copy()\n",
        "\t\t\timg_masked[:, args.img_size//2:] = 0\n",
        "\n",
        "\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n",
        "\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "\tif len(img_batch) > 0:\n",
        "\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "\t\timg_masked = img_batch.copy()\n",
        "\t\timg_masked[:, args.img_size//2:] = 0\n",
        "\n",
        "\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n",
        "\n",
        "mel_step_size = 16\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} for inference.'.format(device))\n",
        "\n",
        "def _load(checkpoint_path):\n",
        "\tif device == 'cuda':\n",
        "\t\tcheckpoint = torch.load(checkpoint_path)\n",
        "\telse:\n",
        "\t\tcheckpoint = torch.load(checkpoint_path,\n",
        "\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\n",
        "\treturn checkpoint\n",
        "\n",
        "def load_model(path):\n",
        "\tmodel = Wav2Lip()\n",
        "\tprint(\"Load checkpoint from: {}\".format(path))\n",
        "\tcheckpoint = _load(path)\n",
        "\ts = checkpoint[\"state_dict\"]\n",
        "\tnew_s = {}\n",
        "\tfor k, v in s.items():\n",
        "\t\tnew_s[k.replace('module.', '')] = v\n",
        "\tmodel.load_state_dict(new_s)\n",
        "\n",
        "\tmodel = model.to(device)\n",
        "\treturn model.eval()\n",
        "\n",
        "def main():\n",
        "\tif not os.path.isfile(args.face):\n",
        "\t\traise ValueError('--face argument must be a valid path to video/image file')\n",
        "\n",
        "\telif args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
        "\t\tfull_frames = [cv2.imread(args.face)]\n",
        "\t\tfps = args.fps\n",
        "\n",
        "\telse:\n",
        "\t\tvideo_stream = cv2.VideoCapture(args.face)\n",
        "\t\tfps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "\t\tprint('Reading video frames...')\n",
        "\n",
        "\t\tfull_frames = []\n",
        "\t\twhile 1:\n",
        "\t\t\tstill_reading, frame = video_stream.read()\n",
        "\t\t\tif not still_reading:\n",
        "\t\t\t\tvideo_stream.release()\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif args.resize_factor > 1:\n",
        "\t\t\t\tframe = cv2.resize(frame, (frame.shape[1]//args.resize_factor, frame.shape[0]//args.resize_factor))\n",
        "\n",
        "\t\t\tif args.rotate:\n",
        "\t\t\t\tframe = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "\t\t\ty1, y2, x1, x2 = args.crop\n",
        "\t\t\tif x2 == -1: x2 = frame.shape[1]\n",
        "\t\t\tif y2 == -1: y2 = frame.shape[0]\n",
        "\n",
        "\t\t\tframe = frame[y1:y2, x1:x2]\n",
        "\n",
        "\t\t\tfull_frames.append(frame)\n",
        "\n",
        "\tprint (\"Number of frames available for inference: \"+str(len(full_frames)))\n",
        "\n",
        "\tif not args.audio.endswith('.wav'):\n",
        "\t\tprint('Extracting raw audio...')\n",
        "\t\tcommand = 'ffmpeg -y -i {} -strict -2 {}'.format(args.audio, 'temp/temp.wav')\n",
        "\n",
        "\t\tsubprocess.call(command, shell=True)\n",
        "\t\targs.audio = 'temp/temp.wav'\n",
        "\n",
        "\twav = audio.load_wav(args.audio, 16000)\n",
        "\tmel = audio.melspectrogram(wav)\n",
        "\tprint(mel.shape)\n",
        "\n",
        "\tif np.isnan(mel.reshape(-1)).sum() > 0:\n",
        "\t\traise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')\n",
        "\n",
        "\tmel_chunks = []\n",
        "\tmel_idx_multiplier = 80./fps \n",
        "\ti = 0\n",
        "\twhile 1:\n",
        "\t\tstart_idx = int(i * mel_idx_multiplier)\n",
        "\t\tif start_idx + mel_step_size > len(mel[0]):\n",
        "\t\t\tmel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
        "\t\t\tbreak\n",
        "\t\tmel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
        "\t\ti += 1\n",
        "\n",
        "\tprint(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
        "\n",
        "\tfull_frames = full_frames[:len(mel_chunks)]\n",
        "\n",
        "\tbatch_size = args.wav2lip_batch_size\n",
        "\tgen = datagen(full_frames.copy(), mel_chunks)\n",
        "\n",
        "\tfor i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n",
        "\t\t\t\t\t\t\t\t\t\t\ttotal=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
        "\t\tif i == 0:\n",
        "\t\t\tmodel = load_model(args.checkpoint_path)\n",
        "\t\t\tprint (\"Model loaded\")\n",
        "\n",
        "\t\t\tframe_h, frame_w = full_frames[0].shape[:-1]\n",
        "\t\t\tout = cv2.VideoWriter('temp/result.avi', \n",
        "\t\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
        "\n",
        "\t\timg_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
        "\t\tmel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tpred = model(mel_batch, img_batch)\n",
        "\n",
        "\t\tpred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
        "\t\t\n",
        "\t\tfor p, f, c in zip(pred, frames, coords):\n",
        "\t\t\ty1, y2, x1, x2 = c\n",
        "\t\t\tp = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
        "\n",
        "\t\t\tf[y1:y2, x1:x2] = p\n",
        "\t\t\tout.write(f)\n",
        "\n",
        "\tout.release()\n",
        "\n",
        "\tcommand = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(args.audio, 'temp/result.avi', args.outfile)\n",
        "\tsubprocess.call(command, shell=platform.system() != 'Windows')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAxD7tZ2a74r"
      },
      "source": [
        "#Flask app"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0gUN1Vaa0dq",
        "outputId": "5ea412c2-e698-4e97-d42f-0ce14c49ef72"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mIdSRGSa4Dh"
      },
      "source": [
        "#output\n",
        "from IPython.display import HTML, clear_output\n",
        "from base64 import b64encode\n",
        "clear_output()\n",
        "outputs = ['/content/Wav2Lip/results/result_voice.mp4']\n",
        "for i,file in enumerate(reversed(outputs)):\n",
        "    if i==len(outputs)-1:\n",
        "      muted = ''\n",
        "    try:\n",
        "      with open(file, 'rb') as f:\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "      display(HTML(\"\"\"\n",
        "      <video width=600 controls autoplay loop %s>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "      </video>\"\"\" % (muted,data_url)))\n",
        "    except Exception:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjsj-knMxSvA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80NPDrSZxFM0"
      },
      "source": [
        "Refer this link to [Flask app](https://roytuts.com/upload-and-display-image-using-python-flask/)\n",
        "\n",
        "index.html\n",
        "\n",
        "\n",
        "```\n",
        "<form action=\"upload\" method=\"post\" id=\"upload-form\">\n",
        "    <input type=\"file\" name=\"imagefile\" id=\"imagefile\"/>\n",
        "    <input type=\"submit\" />\n",
        "  </form>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLopc7vbar19",
        "outputId": "ee6c1536-a214-4a1f-83e9-60a2ec562662"
      },
      "source": [
        "from flask import Flask, request, render_template\n",
        "from flask_ngrok import run_with_ngrok \n",
        "from werkzeug.utils import secure_filename\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import subprocess\n",
        "\n",
        "#sys.path.append(os.path.abspath(\"/content/voca\"))\n",
        "#sys.path.append(os.path.abspath(\"/virtualenvs\"))\n",
        "app = Flask(__name__, template_folder='/content/templates', static_folder=\"/content/static\") \n",
        "run_with_ngrok(app) \n",
        "\n",
        "@app.route(\"/\")\n",
        "def m():\n",
        "  return render_template('index.html')\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload():\n",
        "  try:\n",
        "    filename = request.files.get('imagefile', '')\n",
        "    print(\"Image is uploaded\")\n",
        "    filename = secure_filename(file.filename)\n",
        "    file.save(filename)\n",
        "\t\t#print('upload_image filename: ' + filename)\n",
        "    flash('Image successfully uploaded and displayed below')\n",
        "    return render_template('upload.html', filename=filename)\n",
        "  except Exception as err:\n",
        "    print(err)\n",
        "  return \"<h1>Done</h1>\"\n",
        "\n",
        "def home():\n",
        "  imagefile = flask.request.files.get('imagefile', '')\n",
        "  os.chdir('/content/Wav2Lip')\n",
        "  subprocess.run([\"python\", \"inference.py\"], capture_output=True)\n",
        "\n",
        "  return \"<h2>Video is generated</h2>\"\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://3a2c33e18dd6.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/Mar/2021 11:04:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [23/Mar/2021 11:04:54] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [23/Mar/2021 11:04:55] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [23/Mar/2021 11:05:03] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Image is uploaded\n",
            "'str' object has no attribute 'filename'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdwNPSVBpuYt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}